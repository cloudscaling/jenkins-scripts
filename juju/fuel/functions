#!/bin/bash

my_file="$(readlink -e "$0")"
my_dir="$(dirname $my_file)"

source $my_dir/../functions
source $my_dir/../scaleio/static-checks

# cdir is equal to evn variable WORKSPACE in jenkins and us not equal to my_dir
cdir="$(pwd)"

cloud=${1:-"ec2"}
if [[ "${cloud}" == "gce" ]]; then
  # TODO: Add paths for google.
  instance_type="n1-standard-1"
  device_paths="/var/scaleio_disk"
else
  device_paths="/dev/xvdf"
  rfcache_paths="/dev/xvdh,/dev/xvdi"
fi
username='admin'
machines=()
deploy_counter=1

branch=${BRANCH:-"master"}

trap 'catch_errors $LINENO' ERR

function provision_machines() {
  local new_mx=()
  echo "Provision machines: $*"
  while (( "$#" )); do
    # TODO: Change create machine to use google
    local mx=$(create_machine 2 4)
    echo "Machine $1 created: $mx"
    machines[$1]=${mx}
    new_mx+=(${mx})
    shift
  done

  # wait for machines up
  wait_for_machines ${new_mx[@]}

  # if not EC2:
  #       - prepare device for SDS (truncated file)
  #       - patch host name - in GCE hostname is too long, e.g. juju-023fae94-ef89-4b6d-8681-d9e462c7fae3-machine-2
  if [[ "${cloud}" == "gce" ]]; then
    for i in ${new_mx[@]}; do
      paths=$(echo ${device_paths} | sed 's/,/ /g')
      juju ssh $i "for p in ${paths}; do sudo rm -f \$p && sudo truncate --size 100G \$p; done" 2>/dev/null
      juju ssh $i "sudo sed -i 's/\(juju-\)\(.*\)\(machine.*\)/\1\3/g' /etc/hostname && sudo hostname -F /etc/hostname" 2>/dev/null
    done
  fi

  # check kernel at one machine
  rm -f index.html
  wget -t 2 -T 30 -nv "--ftp-user=QNzgdxXix" "--ftp-password=Aw3wFAwAq3" ftp://ftp.emc.com/Ubuntu/2.0.5014.0/
  kernel=`juju ssh ${new_mx[0]} "uname -r" 2>/dev/null`
  kernel=`echo $kernel | sed 's/\r//'`
  if ! cat index.html | grep $kernel ; then
    echo "WARNING: driver for kernel $kernel not found on ftp.emc.com. Upgrade kernel to 4.2.0-30"

    # change kernel and reboot
    for m in ${new_mx[@]} ; do
      echo "--- Updating machine $m"
      juju ssh $m "sudo apt-get install -fy linux-image-4.2.0-30-generic linux-headers-4.2.0-30-generic &>/dev/null" 2>/dev/null
      juju ssh $m "sudo reboot" 2>/dev/null
    done

    # wait for machines up
    wait_for_machines ${new_mx[@]}
  fi
  rm -f index.html

  # deploy fake module to avoid removal of machines in case of remove last service
  for i in ${new_mx[@]}; do
    juju deploy ubuntu fuel-null-service${i} --to ${i}
  done
}

function copy_log() {
  mch=$1
  log=$2
  dst=$3
  if juju ssh $mch "test -f $log" 2>/dev/null ; then
    juju ssh $mch "cat $log" > "$dst" 2>/dev/null
  fi
}

function save_logs() {
  # save status to file
  for mch in ${machines[@]:1} ; do
    name=fuel-node${mch}
    if juju service get ${name} ; then
      mdir="$cdir/logs/$mch"
      mkdir -p "$mdir"
      copy_log $mch '/var/log/fuel-puppet-scaleio.log' "$mdir/fuel-puppet-scaleio.log"
      copy_log $mch '/var/lib/hiera/defaults.yaml' "$mdir/var-lib-hiera-defaults.log"
    fi
  done
}

function catch_errors() {
  local exit_code=$?
  echo "Line: $1  Error=$exit_code  Command: '$(eval echo $BASH_COMMAND)'"
  trap - ERR
  save_logs
  exit $exit_code
}

function remove_node_service() {
  echo Remove service from machines $@
  for node in $@ ; do
    mch=${machines[$node]}
    name=fuel-node${mch}
    echo "Remove EMC packages from machine $1/$#"
    juju ssh $mch 'sudo truncate --size 0 /etc/environment && sudo apt-get purge -qqyf emc-scaleio-*' 2>/dev/null
    echo "Remove service ${name} from machine $1/$#"
    juju remove-service ${name}
  done
  # wait for remove
  for node in $@ ; do
    mch=${machines[$node]}
    wait_for_removed fuel-node${mch}
  done
}

function prepare_fuel_master() {
  mch=${machines[$1]}
  echo "fuel-master:
    branch-name:  $branch
    device-paths: $device_paths" >/tmp/config.yaml
  juju deploy --repository fuel-charms local:trusty/fuel-master --to $mch --config /tmp/config.yaml
}

function set_fuel_options() {
  juju set fuel-master $*
}

function deploy_node_service() {
  mch=${machines[$1]}
  name=fuel-node${mch}
  roles=$2

  if ! juju service get ${name} &>/dev/null  ; then
    echo "$name:
      branch-name:  $branch" >/tmp/config.yaml
    juju deploy --repository fuel-charms local:trusty/fuel-node $name --to $mch --config /tmp/config.yaml
    juju add-relation fuel-master $name
  fi
  juju set $name roles="${roles}"
}

function configure_cluster() {
  mdms=""
  while (( "$#" )); do
    case "$1" in
      "compute")
        shift
        for i in `echo $1 | sed 's/,/ /g'`; do
          deploy_node_service $i "compute"
        done
      ;;
      "primary-controller")
        shift
        deploy_node_service $1 "primary-controller cinder"
        mdms+=" ${machines[$1]}"
      ;;
      "controller")
        shift
        for i in `echo $1 | sed 's/,/ /g'`; do
          deploy_node_service $i "controller cinder"
          mdms+=" ${machines[$i]}"
        done
      ;;
      "mode")
        shift
        mode=$1
      ;;
    esac
    shift
  done

  echo "Wait for services start: $(date)"
  wait_absence_status_for_services "executing|blocked|waiting|allocating"
  echo "Wait for services end: $(date)"

  juju set fuel-master deploy=$deploy_counter
  ((deploy_counter++))

  echo "Wait for services start: $(date)"
  wait_absence_status_for_services "maintenance"
  echo "Wait for services end: $(date)"

  # check for errors
  if juju status | grep "current" | grep error ; then
    echo "ERROR: Some services went to error state"
    juju ssh 0 sudo grep Error /var/log/juju/all-machines.log 2>/dev/null
    return 1
  fi

  metadata_enable=`get_config fuel-master metadata-enable`
  existing_cluster=`get_config fuel-master existing-cluster`
  if [[ $metadata_enable == '"true"' && $existing_cluster == '"false"' ]] ; then
    # check query_cluster output before exit on error if exists
    master_mdm=`get_master_mdm "echo $mdms"`
    echo "INFO: query cluster on machine $master_mdm"
    juju ssh $master_mdm 'scli --query_cluster --approve_certificate' 2>/dev/null

    check-cluster "juju ssh" $master_mdm $mode
  fi
}

#TODO: remove duplication of code: re-use the check functions from common code when it appears
function get_pd_list {
  controller=$1
  password=$2
  login_cmd="scli --login --username $username --password $password --approve_certificate"
  pd_names_cmd="scli --query_properties --all_objects --object_type PROTECTION_DOMAIN --properties NAME"

  pd_output=`juju ssh $mch "$login_cmd && $pd_names_cmd" 2>/dev/null `
  echo "$pd_output" | awk '/NAME/{print$2}' | sed 's/\r//g'
}

function check_storage_pool {
  mch=${machines[$1]}
  key_to_check=$2
  expected_value=$3

  storage_pools=`get_config fuel-master storage-pools`
  sp_list=`echo "$storage_pools" | sed 's/,/ /g'`

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"

  pd_list=`get_pd_list $mch $password`

  ret=0
  for pd in $pd_list ; do
    for sp in $sp_list ; do
      query_cmd="scli --query_storage_pool --protection_domain_name $pd --storage_pool_name $sp"
      output=`juju ssh $mch "$login_cmd && $query_cmd" 2>/dev/null`
      if echo "$output" | grep "$key_to_check" | grep -q "$expected_value" ; then
        echo "INFO: Success. Parameter $key_to_check in storage pool $sp in protection domain $pd is $expected_value."
      else
        echo "ERROR: Parameter $key_to_check in storage pool $sp in protection domain $pd is in wrong state."
        echo "$output"
        ((++ret))
      fi
    done
  done

  return $ret
}

function check_specific_storage_pool {
  mch=${machines[$1]}
  key_to_check=$2
  expected_value=$3
  storage_pools=`get_config fuel-master storage-pools`
  checked_sps=${4:-"$storage_pools"}

  sp_list=`echo "$storage_pools" | sed 's/,/ /g'`
  checked_sp_list=(`echo "$checked_sps" | sed 's/,/ /g'`)

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"

  pd_list=`get_pd_list $mch $password`

  ret=0
  for pd in $pd_list ; do
    for sp in $sp_list ; do
      query_cmd="scli --query_storage_pool --protection_domain_name $pd --storage_pool_name $sp"
      output=`juju ssh $mch "$login_cmd && $query_cmd" 2>/dev/null`
      if [[ ${checked_sp_list[@]} =~ "$sp" ]] ; then
        if echo "$output" | grep "$key_to_check" | grep -q "$expected_value" ; then
          echo "INFO: Success. Parameter $key_to_check in storage pool $sp in protection domain $pd is $expected_value."
        else
          echo "ERROR: Parameter $key_to_check in storage pool $sp in protection domain $pd is in wrong state."
          echo "$output"
          ((++ret))
        fi
      else
        if ! echo "$output" | grep "$key_to_check" | grep -q "$expected_value" ; then
          echo "INFO: Success. Parameter $key_to_check in storage pool $sp in protection domain $pd is not in state $expected_value."
        else
          echo "ERROR: Parameter $key_to_check in storage pool $sp in protection domain $pd is in $expected_value, but shouldn't."
          echo "$output"
          ((++ret))
        fi
      fi
    done
  done

  return $ret
}

function check_capacity_alerts {
  mch=${machines[$1]}
  high_alert=$2
  critical_alert=$3

  storage_pools=`get_config fuel-master storage-pools`
  sp_list=`echo "$storage_pools" | sed 's/,/ /g'`

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"

  pd_list=`get_pd_list $mch $password`

  ret=0
  for pd in $pd_list ; do
    for sp in $sp_list ; do
      query_cmd="scli --query_storage_pool --protection_domain_name $pd --storage_pool_name $sp"
      output_storage_pool=`juju ssh $mch "$login_cmd && $query_cmd" 2>/dev/null | grep "Capacity alert thresholds"`
      if echo "$output_storage_pool" | awk '{print$5}' | grep -q "$high_alert" ; then
         echo "INFO: Success. Capacity alert thresholds on $sp storage pool in protection domain $pd: high is $high_alert."
      else
        echo "ERROR: Wrong capacity high alert thresholds on $sp storage pool in protection domain $pd. Expected $high_alert."
        echo "$output_storage_pool"
        ((++ret))
      fi
      if echo "$output_storage_pool" | awk '{print$7}' | grep -q "$critical_alert" ; then
        echo "INFO: Success. Capacity alert thresholds on $sp storage pool in protection domain $pd: critical is $critical_alert"
      else
        echo "ERROR: Wrong capacity critical alert thresholds on $sp storage pool in protection domain $pd. Expected $critical_alert."
        echo "$output_storage_pool"
        ((++ret))
      fi
    done
  done

  return $ret
}

function check_rfcache {
  mch=${machines[$1]}
  rfcache_list=(`echo $2 | sed 's/,/ /g'`)
  rfcache_count=${#rfcache_list[@]}

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"
  sds_names=(`juju ssh $mch "$login_cmd && scli --query_all_sds" 2>/dev/null| grep 'SDS ID:' | awk '{print$5}'`)

  ret=0
  for sds_name in ${sds_names[@]} ; do
    rfcache_output=`juju ssh $mch "$login_cmd && scli --query_sds --sds_name $sds_name" 2>/dev/null | grep "Rfcache device information" -A $(($rfcache_count*2+1))`
    if ! echo "$rfcache_output" | grep -q "total $rfcache_count devices"  ; then
      echo "ERROR: Unexpected number of devices on $sds_name."
      echo "$rfcache_output" | grep "Rfcache device information"
      ((++ret))
    fi
    for rfcache in ${rfcache_list[@]} ; do
      if ! echo "$rfcache_output" | grep -q $rfcache ; then
        echo "ERROR: No $rfcache RFCache path on $sds_name"
        echo "$rfcache_output"
        ((++ret))
      else
        echo "INFO: Success. There is $rfcache RFCache path of on $sds_name."
      fi
    done
  done

  return $ret
}

function check_protection_domain {
  mch=${machines[$1]}
  expected_pd_name=$2

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"

  sds_names=(`juju ssh $mch "$login_cmd && scli --query_all_sds" 2>/dev/null | grep 'SDS ID:' | awk '{print$5}'`)

  ret=0
  for sds_name in ${sds_names[@]} ; do
    output_sds=`juju ssh $mch "$login_cmd && scli --query_sds --sds_name $sds_name" 2>/dev/null`
    current_pd_name=`echo "$output_sds" | awk '/Protection Domain:/{print$5}' | sed 's/\r//g'`
    if [[ "${current_pd_name%%_*}" == "${expected_pd_name}" ]] ; then
      echo "INFO: Success. Protection domain on $sds_name is $current_pd_name."
    else
      echo "ERROR: Wrong protection domain on $sds_name."
      echo "$output_sds" | grep "Protection Domain:"
      ((++ret))
    fi
  done

  return $ret
}

function check_password {
  mch=${machines[$1]}
  password=$2

  ret=0
  if juju ssh $mch "scli --login --username $username --password $2 --approve_certificate" 2>/dev/null ; then
    echo "INFO: Success. Password was changed to $password"
  else
    echo "ERROR: Wrong password. Please check."
    ((++ret))
  fi

  return $ret
}

function check_sds_storage_pool {
  mch=${machines[$1]}
  sp_list=(`echo $2 | sed 's/,/ /g'`)
  device_paths_list=(`echo $3 | sed 's/,/ /g'`)

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"
  sds_names=(`juju ssh $mch "$login_cmd && scli --query_all_sds" 2>/dev/null | grep 'SDS ID:' | awk '{print$5}'`)

  ret=0
  for sds_name in ${sds_names[@]} ; do
    output_sds=`juju ssh $mch "$login_cmd && scli --query_sds --sds_name $sds_name" 2>/dev/null`
    if ! echo "$output_sds" | grep "Device information" | grep -q "total ${#device_paths_list[@]} devices" ; then
      echo "ERROR: Error in devices number in $sds_name"
      echo "$output_sds"
      ((++ret))
    fi
    i=0
    for devpath in $device_paths_list ; do
      if ! device_info=`echo "$output_sds" | grep -A 2 "$devpath"`  ; then
        echo "ERROR: Not found expected $devpath device path on $sds_name."
        echo "$output_sds"
        ((++ret))
      elif ! echo "$device_info" | grep -q ${sp_list[$i]} ; then
        echo "ERROR: Wrong storage pool on $devpath on $sds_name. Expected ${sp_list[$i]}."
        echo "$device_info"
        ((++ret))
      fi
      echo "INFO: Success. There is ${sp_list[$i]} storage pool on $devpath on $sds_name."
      ((++i))
     done
  done

  return $ret
}

function check_fuel_performance {
  mch=${machines[$1]}
  password=`get_config fuel-master password`

  check-performance "juju ssh" "$mch" "$username" "$password"
}

function check_sds_on_controller {
  mch=${machines[$1]}
  expected=$2

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"

  #TODO: change check in case there are several mdms
  mdm_ip=`juju ssh $mch "scli --query_cluster | grep 'Management IPs'" 2>/dev/null | awk '{print$5}' | sed 's/,//g'`
  sds_on_controller='false'
  all_sds=`juju ssh $mch "$login_cmd && scli --query_all_sds" 2>/dev/null`
  if echo "$all_sds" | grep $mdm_ip ; then
    sds_on_controller='true'
  fi

  if [[ $sds_on_controller == $expected ]] ; then
    echo "INFO: Success. SDS on controller is $expected."
  else
    echo "ERROR: SDS on controller is $sds_on_controller, but is expected to be $expected."
    return 1
  fi
}

function check_protection_domain_nodes {
  mch=${machines[$1]}
  max_nodes=$2

  password=`get_config fuel-master password`

  login_cmd="scli --login --username $username --password $password --approve_certificate"
  query_cmd="scli --query_properties --all_objects --object_type PROTECTION_DOMAIN --properties NAME,NUM_OF_SDS"
  pd_output=`juju ssh $mch "$login_cmd && $query_cmd" 2>/dev/null `
  names_of_pd=(`echo "$pd_output" | awk '/NAME/{print$2}' | sed "s/\r//g"`)
  num_of_sds=`echo "$pd_output"  | awk '/NUM_OF_SDS/{print$2}' | sed "s/\r//g"`
  ret=0
  i=0
  sds_count=0
  for sds in $num_of_sds ; do
    sds_count=$((sds_count+sds))
    pd_name=${names_of_pd[$i]}
    if (( $sds > $max_nodes )) ; then
      echo "ERROR: Wrong number of SDS in protection domain $pd_name. Expected maximum $max_nodes, but got $sds."
      ((++ret))
    else
      echo "INFO: Success. $sds SDSs in protection domain $pd_name. Expected maximum $max_nodes."
    fi
    ((++i))
  done

  maximum_sds_for_less_pd=$(((${#names_of_pd[@]}-1)*${max_nodes}))
  if (( $maximum_sds_for_less_pd < $sds_count )) ; then
    echo "INFO: Success. Number of protection domains is optimal."
  else
    echo "ERROR: Too much protection domains was created."
    ((++ret))
  fi

  return $ret
}

function check_scaleio_not_installed {
  mch=${machines[$1]}

  if juju ssh $mch "dpkg -l emc*" &>/dev/null ; then
    echo "ERROR: ScaleIO packages was installed to controller, but should not."
    return 1
  else
    echo "INFO: Success. ScaleIO packages wasn't installed to controller."
  fi
}

function check_branch {
  mch=${machines[$1]}

  branch=`get_config fuel-master branch-name`

  master_file=' deployment_tasks.yaml'
  branch_file=' tasks.yaml'

  files="sudo ls -l /root/fuel-plugin-scaleio"

  if [[ $branch == "master" ]] ; then
    present_file=$master_file
    absent_file=$branch_file
  else
    present_file=$branch_file
    absent_file=$master_file
  fi

  if juju ssh $mch "$files" 2>/dev/null | grep "$present_file" ; then
    if ! juju ssh $mch "$files" 2>/dev/null | grep "$absent_file" ; then
      echo "INFO: Success. Your branch is $branch. All files are corresponding."
    else
      echo "ERROR: Your branch is $branch. There is unexpected file $absent_file in /root/fuel-plugin-scaleio."
      return 1
    fi
  else
    echo "ERROR: Your branch is $branch. There is no expected file $present_file in /root/fuel-plugin-scaleio."
    return 1
  fi
}

function check_existing_cluster {
  mdm=`get_master_mdm`
  sdc_nodes=`echo $1 | sed 's/,/ /g'`

  cluster_mdm_ips=(`juju ssh $mdm "scli --query_cluster --approve_certificate" 2>/dev/null | awk '/Management IPs/{print $5}' | sed 's/,//g'`)

  ret=0
  for node in $sdc_nodes ; do
    sdc=${machines[$node]}
    #check emc-packages"
    if output=`juju ssh $sdc "dpkg -l | grep 'emc-'" 2>/dev/null | grep -v 'scaleio-sdc'` ; then
      echo "ERROR: There is scaleio packages on machine $sdc besides scaleio-sdc."
      echo $output
      ((++ret))
    else
      echo "INFO: Success. There is no scaleio packages on machine $sdc besides scaleio-sdc."
    fi

    sdc_mdm_ips=(`juju ssh $sdc "sudo /bin/emc/scaleio/drv_cfg --query_mdms" 2>/dev/null | awk '/MDM-ID/{print $10}' | sed 's/\[0\]-//g'`)

    #check MDM addresses"
    mdm_err=0
    if [[ ${#cluster_mdm_ips[@]} != ${#sdc_mdm_ips[@]} ]] ; then
      echo "ERROR: Wrong MDM number on SDC node $sdc. Expected ${#cluster_mdm_ips[@]}, but got ${#sdc_mdm_ips[@]}."
      ((++mdm_err))
    else
      for ip in $cluster_mdm_ips ; do
        if ! [[ ${sdc_mdm_ips[@]} =~ $ip ]] ; then
          echo "ERROR: There is no MDM with ip $ip in cluster, but it presents on machine $sdc."
          ((++mdm_err))
        fi
      done
    fi
    if (( mdm_err == 0 )) ; then
      echo "INFO: Success. The SDC node $sdc has correct information about MDMs in cluster."
    else
      ((++ret))
    fi

    juju ssh $sdc "sudo apt-get install emc-scaleio-mdm" 2>/dev/null

    mdm_ip=${cluster_mdm_ips[0]}

    password=`get_config fuel-master password`
    login_cmd="scli --mdm_ip $mdm_ip --login --username $username --password $password --approve_certificate"

    pd_name=`juju ssh $mdm "$login_cmd && scli --query_properties --all_objects --object_type PROTECTION_DOMAIN --properties NAME" 2>/dev/null | awk '/NAME/{print $2}' | sed 's/\r//g'`
    sp_name=`juju ssh $mdm "$login_cmd && scli --query_properties --all_objects --object_type STORAGE_POOL --properties NAME" 2>/dev/null | awk '/NAME/{print $2}' | sed 's/\r//g'`

    #create volume
    volume_name=vol$sdc
    add_volume_cmd="scli --mdm_ip $mdm_ip --add_volume --protection_domain_name $pd_name --storage_pool_name $sp_name --size_gb 8 --volume_name $volume_name"
    if ! output=`juju ssh $sdc "$login_cmd && $add_volume_cmd" 2>/dev/null` ; then
      echo "ERROR: Couldn't create volume $volume_name on SDC node $sdc."
      echo "$output"
      ((++ret))
      continue
    fi

    #map volume
    sdc_ip=`juju ssh $sdc "ip addr show dev eth0" 2>/dev/null | awk '/inet /{split($2,a,"/"); print a[1]}'`
    map_volume_cmd="scli --mdm_ip $mdm_ip --map_volume_to_sdc --volume_name $volume_name --sdc_ip $sdc_ip"
    if ! output=`juju ssh $sdc "$login_cmd && $map_volume_cmd" 2>/dev/null` ; then
      echo "ERROR: Couldn't map volume $volume_name on SDC node $sdc."
      echo "$output"
      ((++ret))
      continue
    fi

    #check volumes
    query_cmd="scli --mdm_ip $mdm_ip --query_all_volumes"
    if juju ssh $sdc "$login_cmd && $query_cmd" 2>/dev/null | grep $volume_name | grep "Mapped to" ; then
      echo "INFO: Success. The volume $volume_name was created and mapped on SDC node $sdc."
    else
      echo "ERROR: There is no mapped volume $volume_name in cluster."
      ((++ret))
    fi
  done

  if (( ret == 0 )) ; then
    echo "INFO: Success. All SDC nodes are connected to existing cluster."
  else
    echo "ERROR: Not all nodes passed all checks."
    return 1
  fi
}

